{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_with_training_module_12/28.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjapan87/ChatbotImplement_V1/blob/main/chatbot_with_training_module_12_28.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsA55M9bYjgf"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "import re \n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkZrAd_P12Lu",
        "outputId": "d782d5a6-7ec9-469b-b4ac-4b006300cb9c"
      },
      "source": [
        "!pip install tensorflow==1.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.0.0 in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.19.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow==1.0.0) (51.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QzR4-abY0HE"
      },
      "source": [
        "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H41GuSzTY3Aq"
      },
      "source": [
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x-dhn5NY58w"
      },
      "source": [
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag2NvvEKY9Jr"
      },
      "source": [
        "questions = []\n",
        "answers = []\n",
        "for conversation in conversations_ids:\n",
        "  for i in range(len(conversation) - 1):\n",
        "    questions.append(id2line[conversation[i]])\n",
        "    answers.append(id2line[conversation[i+1]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1VqPDN0Y_4x"
      },
      "source": [
        "def cleanText(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"we'd\", \"we would\", text)    \n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYcS3OEBZCzZ"
      },
      "source": [
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(cleanText(question))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXIHADd-ZF_v"
      },
      "source": [
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(cleanText(answer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiXAvBI4ZJLA"
      },
      "source": [
        "word2count = {}\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "          word2count[word] = 1\n",
        "        else:\n",
        "          word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZTcpBjZZM8n"
      },
      "source": [
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "          word2count[word] = 1\n",
        "        else:\n",
        "          word2count[word] += 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Seu1Q4IZPcK"
      },
      "source": [
        "threshold_ques = 20\n",
        "word_number = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKcs8n2jZRqI"
      },
      "source": [
        "question2int_dict = {}\n",
        "for word,count in word2count.items():\n",
        "    if count >= threshold_ques:\n",
        "        question2int_dict[word] = word_number\n",
        "        word_number += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZTKwuOMZTuU"
      },
      "source": [
        "threshold_ans = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5d_pFCvZWxZ"
      },
      "source": [
        "answers2int_dict = {}\n",
        "for word,count in word2count.items():\n",
        "    if count >= threshold_ans:\n",
        "        answers2int_dict[word] = word_number\n",
        "        word_number += 1    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6iJBFQKZZER"
      },
      "source": [
        "tokens = [\"<PAD>\",\"<EOS>\",\"<OUT>\",\"<SOS>\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn5Rx7PNZbbS"
      },
      "source": [
        "for token in tokens:\n",
        "    question2int_dict[token] = len(question2int_dict)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5AFgKX4Zd8c"
      },
      "source": [
        "for token in tokens:\n",
        "    answers2int_dict[token] = len(answers2int_dict)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLrO9XzzZgCJ"
      },
      "source": [
        "answers2int_wordDict = {word_i:word for word,word_i in answers2int_dict.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXqygJDBZiYS"
      },
      "source": [
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] += \" <EOS>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKgs1Nm-ZlFp"
      },
      "source": [
        "out_questions = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWTvi3QgZnjq"
      },
      "source": [
        "for question in clean_questions:\n",
        "  integ=[] #using full-sentences & using the created dictionarie\n",
        "  for word in question.split():\n",
        "    if word not in question2int_dict:\n",
        "      integ.append(question2int_dict['<OUT>'])\n",
        "    else:\n",
        "      integ.append(question2int_dict[word])\n",
        "  out_questions.append(integ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNlaUXeoZqac"
      },
      "source": [
        "out_answers = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJpICTWIZt5K"
      },
      "source": [
        "for answer in clean_answers:\n",
        "  integ=[] #using full-sentences & using the created dictionaries\n",
        "  for word in answer.split():\n",
        "    if word not in answers2int_dict:\n",
        "      integ.append(answers2int_dict['<OUT>'])\n",
        "    else:\n",
        "      integ.append(answers2int_dict[word])\n",
        "  out_answers.append(integ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aivPky1yaH0-"
      },
      "source": [
        "sorted_out_question = []\n",
        "sorted_out_answers = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lprw1rWhaNMQ"
      },
      "source": [
        "for length in range(1, 10+1):\n",
        "    for i in enumerate(out_questions):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_out_question.append(out_questions[i[0]])\n",
        "            sorted_out_answers.append(out_answers[i[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbhwVQDGaPiC"
      },
      "source": [
        "def model_input():\n",
        "    inputs = tf.placeholder(tf.int32,[None,None],name = \"input\")\n",
        "    target = tf.placeholder(tf.int32,[None,None],name = \"output\")\n",
        "    LR = tf.placeholder(tf.float32,name = \"learning_rate\")\n",
        "    keep_prob = tf.placeholder(tf.float32,name = \"dropout\")\n",
        "    return inputs,target,LR,keep_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsvFE6aaaSH7"
      },
      "source": [
        "def preprocess (target,word2int,batch_size):\n",
        "    left_side = tf.fill([batch_size,1], word2int[\"<SOS>\"], name=None)\n",
        "    right_side = tf.strided_slice(input = target, \n",
        "                                  begin = [0,0], \n",
        "                                  end = [batch_size,-1], \n",
        "                                  strides=[1,1], \n",
        "                                  begin_mask=0, \n",
        "                                  end_mask=0, \n",
        "                                  ellipsis_mask=0,\n",
        "                                  new_axis_mask=0, \n",
        "                                  shrink_axis_mask=0, \n",
        "                                  var=None, name=None)\n",
        "    pre_process_targets =tf.concat(values = [left_side,right_side], axis = 1)\n",
        "    return pre_process_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfrpYh-9abPT"
      },
      "source": [
        "def encoder (rnn_input,\n",
        "             rnn_size,\n",
        "             num_layers,\n",
        "             keep_prob,\n",
        "             sequence_length):\n",
        "    create_LSTM = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "    LSTM_dropout = tf.contrib.rnn.DropoutWrapper(create_LSTM,input_keep_prob = keep_prob)\n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([LSTM_dropout]*num_layers)\n",
        "    _,encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw= encoder_cell, \n",
        "                                                      cell_bw= encoder_cell,\n",
        "                                                      sequence_length = sequence_length,\n",
        "                                                      inputs = rnn_input,\n",
        "                                                      dtype = tf.float32)\n",
        "    return encoder_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3SlmF2Haevt"
      },
      "source": [
        "def decoder_training_data (encoder_state, \n",
        "                           decoder_cell, \n",
        "                           decoder_embedding_input, \n",
        "                           sequence_length, \n",
        "                           decoding_scope, \n",
        "                           output_function, \n",
        "                           keep_prob , \n",
        "                           batch_size):\n",
        "    \n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size], \n",
        "                                dtype=tf.dtypes.float32, \n",
        "                                name=None) \n",
        "    \n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function \\\n",
        "        = tf.contrib.seq2seq.prepare_attention(attention_states,\n",
        "                                               attention_option = \"Bahdanau\",\n",
        "                                               num_units = decoder_cell.output_size,\n",
        "                                               reuse=False)\n",
        "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0], \n",
        "                                                                        attention_keys, \n",
        "                                                                        attention_values, \n",
        "                                                                        attention_score_function, \n",
        "                                                                        attention_construct_function, \n",
        "                                                                        name=\"AttentionDecoderTraining\") \n",
        "    \n",
        "    decoder_output,_,_ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell, \n",
        "                                                                training_decoder_function, \n",
        "                                                                decoder_embedding_input, \n",
        "                                                                sequence_length, \n",
        "                                                                scope = decoding_scope, \n",
        "                                                                name=\"DecoderOutputforTraining\")\n",
        "    decoder_dropout = tf.nn.dropout(decoder_output,\n",
        "                                    keep_prob,\n",
        "                                    name=\"DecoderDropoutController\")\n",
        "    \n",
        "    return output_function(decoder_dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9IDbNB6ayIy"
      },
      "source": [
        "def decoder_testing_data (encoder_state, \n",
        "                          decoder_cell, \n",
        "                          decoder_embedding_matrix, \n",
        "                          sos_id, \n",
        "                          eos_id, \n",
        "                          maximum_length, \n",
        "                          num_words, \n",
        "                          decoding_scope, \n",
        "                          output_function, \n",
        "                          keep_prob , \n",
        "                          batch_size):\n",
        "    \n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size], \n",
        "                                dtype=tf.dtypes.float32, \n",
        "                                name=None) \n",
        "    \n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function \\\n",
        "        = tf.contrib.seq2seq.prepare_attention(attention_states, \n",
        "                                               attention_option = \"bahdanau\", \n",
        "                                               num_units = decoder_cell.output_size, \n",
        "                                               reuse=False) \n",
        "        \n",
        "    testing_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function, \n",
        "                                                                           encoder_state[0], \n",
        "                                                                           attention_keys, \n",
        "                                                                           attention_values, \n",
        "                                                                           attention_score_function, \n",
        "                                                                           attention_construct_function, \n",
        "                                                                           decoder_embedding_matrix, \n",
        "                                                                           sos_id, \n",
        "                                                                           eos_id, \n",
        "                                                                           maximum_length, \n",
        "                                                                           num_words, \n",
        "                                                                           dtype=tf.int32, \n",
        "                                                                           name=None) \n",
        "      \n",
        "    test_predictions,_,_ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell, \n",
        "                                                                  testing_decoder_function, \n",
        "                                                                  scope = decoding_scope, \n",
        "                                                                  name=\"DecoderOutputforTesting\")\n",
        "    return test_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg8f-ROXa9Hz"
      },
      "source": [
        "def decoder (decoder_embedding_input,\n",
        "             decoder_embedding_matrix, \n",
        "             encoder_state, \n",
        "             num_words, \n",
        "             rnn_size, \n",
        "             num_layers, \n",
        "             word2int, \n",
        "             keep_prob, \n",
        "             sequence_length, \n",
        "             batch_size):\n",
        "  with tf.variable_scope(\"decoding\") as decoding_scope:\n",
        "    create_LSTM = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "        \n",
        "    LSTM_dropout = tf.contrib.rnn.DropoutWrapper(create_LSTM,input_keep_prob = keep_prob)\n",
        "        \n",
        "    decoder_cell = tf.contrib.rnn.MultiRNNCell([LSTM_dropout]*num_layers)\n",
        "    \n",
        "    weights = tf.truncated_normal_initializer(mean=0.0,\n",
        "                                                  stddev=1.0,\n",
        "                                                  seed=None,\n",
        "                                                  dtype=tf.dtypes.float32)\n",
        "    bias = tf.zeros_initializer()\n",
        "        \n",
        "    output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
        "                                                                      num_words,\n",
        "                                                                      activation_fn=tf.nn.relu,\n",
        "                                                                      normalizer_fn=None,\n",
        "                                                                      normalizer_params=None,\n",
        "                                                                      weights_initializer= weights,\n",
        "                                                                                      weights_regularizer=None,\n",
        "                                                                                      biases_initializer=bias,\n",
        "                                                                                      biases_regularizer=None,\n",
        "                                                                                      reuse=None,\n",
        "                                                                                      variables_collections=None,\n",
        "                                                                                      outputs_collections=None,\n",
        "                                                                                      trainable=True,\n",
        "                                                                                      scope=decoding_scope)\n",
        "    training_predictions = decoder_training_data (encoder_state, \n",
        "                                                      decoder_cell, \n",
        "                                                      decoder_embedding_input, \n",
        "                                                      sequence_length, \n",
        "                                                      decoding_scope, \n",
        "                                                      output_function,\n",
        "                                                      keep_prob , \n",
        "                                                      batch_size)\n",
        "    decoding_scope.reuse_variables()\n",
        "\n",
        "    testing_predictions = decoder_testing_data (encoder_state, \n",
        "                                                    decoder_cell, \n",
        "                                                    decoder_embedding_matrix, \n",
        "                                                    word2int[\"<SOS>\"], \n",
        "                                                    word2int[\"<EOS>\"],   \n",
        "                                                    num_words, \n",
        "                                                    sequence_length-1, \n",
        "                                                    decoding_scope, \n",
        "                                                    output_function, \n",
        "                                                    keep_prob , \n",
        "                                                    batch_size)\n",
        "        \n",
        "    return training_predictions,testing_predictions\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ZhZTGRbx7b"
      },
      "source": [
        "def seq2seq (inputs,\n",
        "             target,\n",
        "             keep_prob,\n",
        "             sequence_length, \n",
        "             batch_size, \n",
        "             questions_number_words,\n",
        "             answers_number_words,\n",
        "             encoder_embedding_size,\n",
        "             decoder_embedding_size,\n",
        "             rnn_size,\n",
        "             num_layers,\n",
        "             question2int_dict):\n",
        "  encoder_embedded_input=tf.contrib.layers.embed_sequence(inputs,\n",
        "                                                          answers_number_words+1,\n",
        "                                                          encoder_embedding_size,\n",
        "                                                          initializer=tf.random_uniform_initializer(minval=0, maxval=1,seed=None)\n",
        "                                                          )\n",
        "  encoder_state = encoder(encoder_embedded_input, \n",
        "                            rnn_size,\n",
        "                            num_layers,\n",
        "                            keep_prob,\n",
        "                            sequence_length)\n",
        "  pre_process_targets = preprocess(target, question2int_dict, batch_size)\n",
        "  \n",
        "  decoder_embedding_matrix= tf.Variable(tf.random.uniform([questions_number_words+1,decoder_embedding_size],\n",
        "                                                          minval=0,\n",
        "                                                          maxval=1,\n",
        "                                                          dtype=tf.dtypes.float32,\n",
        "                                                          seed=None,\n",
        "                                                          name=None))\n",
        "  \n",
        "  decoder_embedding_input = tf.nn.embedding_lookup(decoder_embedding_matrix,\n",
        "                                                   pre_process_targets,\n",
        "                                                   max_norm=None,name=None)\n",
        "  \n",
        "  training_predictions,testing_predictions \\\n",
        "        = decoder (decoder_embedding_input,\n",
        "                   decoder_embedding_matrix, \n",
        "                   encoder_state, \n",
        "                   questions_number_words, \n",
        "                   rnn_size, \n",
        "                   num_layers, \n",
        "                   question2int_dict, \n",
        "                   keep_prob, \n",
        "                   sequence_length, \n",
        "                   batch_size)\n",
        "    \n",
        "  return training_predictions,testing_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgUV_IFsdQ_e"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 64\n",
        "rnn_size = 512 ##no. of neurons in 1 RNN cell\n",
        "num_layers = 3\n",
        "encoder_embedding_size = 512\n",
        "decoder_embedding_size = 512\n",
        "learning_rate = 0.01 \n",
        "learning_rate_decay = 0.90 \n",
        "min_learning_rate = 0.0001\n",
        "keep_probability = 0.50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhjGTyK-dZy1"
      },
      "source": [
        "#tf.compat.v1.disable_eager_execution()\n",
        "tf.reset_default_graph()\n",
        "session = tf.InteractiveSession()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgvlQCRndfx1"
      },
      "source": [
        "inputs,target,LR,keep_prob = model_input()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um24OebcdndS"
      },
      "source": [
        "sequence_length = tf.placeholder_with_default(25,[None,None,1,1], name='sequence_length')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3sDinfwdoYc"
      },
      "source": [
        "input_shape = tf.shape(inputs)\n",
        "#out_type=tf.dtypes.int32, name=None\n",
        "len_question = len(question2int_dict)\n",
        "len_answer = len(answers2int_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abJxKnEyelOE"
      },
      "source": [
        "# ***ERROR NEXT CELL***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RyL6cJwdtMu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "a52d1180-6b79-45fc-c9bb-af91e1be651b"
      },
      "source": [
        "training_predictions,testing_predictions\\\n",
        "    =seq2seq (tf.reverse(inputs,[-1]),\n",
        "              target,\n",
        "              keep_prob,\n",
        "              batch_size, \n",
        "              sequence_length,\n",
        "              len(answers2int_dict),\n",
        "              len(question2int_dict),\n",
        "              encoder_embedding_size,\n",
        "              decoder_embedding_size,\n",
        "              rnn_size,\n",
        "              num_layers,\n",
        "              question2int_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-259-3b628d677893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m               \u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m               \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m               question2int_dict)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-253-cfccdea7c1a7>\u001b[0m in \u001b[0;36mseq2seq\u001b[0;34m(inputs, target, keep_prob, sequence_length, batch_size, questions_number_words, answers_number_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, question2int_dict)\u001b[0m\n\u001b[1;32m     20\u001b[0m                             \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                             \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                             sequence_length)\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mpre_process_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion2int_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-249-43cd1625971f>\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(rnn_input, rnn_size, num_layers, keep_prob, sequence_length)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                       \u001b[0msequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                                       dtype = tf.float32)\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoder_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mbidirectional_dynamic_rnn\u001b[0;34m(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    348\u001b[0m           \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m           \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m           time_major=time_major, scope=fw_scope)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# Backward direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    495\u001b[0m       raise ValueError(\n\u001b[1;32m    496\u001b[0m           \u001b[0;34m\"sequence_length must be a vector of length batch_size, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           \"but saw shape: %s\" % sequence_length.get_shape())\n\u001b[0m\u001b[1;32m    498\u001b[0m     sequence_length = array_ops.identity(  # Just to find it in the graph.\n\u001b[1;32m    499\u001b[0m         sequence_length, name=\"sequence_length\")\n",
            "\u001b[0;31mValueError\u001b[0m: sequence_length must be a vector of length batch_size, but saw shape: ()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1MxFHgPeqAN"
      },
      "source": [
        "with tf.name_scope(\"Optimization\"):\n",
        "  loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
        "                                         target,\n",
        "                                         tf.ones([input_shape[0],sequence_length])\n",
        "                                         )\n",
        "  Optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "  Gradient = Optimizer.compute_gradients(loss_error)\n",
        "  Clipped_Gradient = [(tf.clip_by_value(grad_tensor,-5.0,5.0),grad_variable) for grad_tensor,grad_variable in Gradient if grad_tensor is not None]\n",
        "  Optimizer_Clipped_Gradient = Optimizer.apply_gradients(Clipped_Gradient)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8n3WYhwe67z"
      },
      "source": [
        "def apply_padding(batch_of_sequences, word2int):\n",
        "  maximum_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
        "  return [sequence + [word2int[\"<PAD>\"]]*(maximum_sequence_length-len(sequence)) for sequence in batch_of_sequences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-KvTgB0e_UQ"
      },
      "source": [
        "def splits_into_batches(questions,answers,batch_size):\n",
        "  for batch_index in range(0,len(questions)//batch_size):\n",
        "    start_index = batch_index*batch_size\n",
        "    question_in_batch = questions[start_index:start_index+batch_size]\n",
        "    answers_in_batch  = answers[start_index:start_index+batch_size]\n",
        "    padded_question_in_batch = np.array(apply_padding(question_in_batch, question2int_dict))\n",
        "    padded_answer_in_batch = np.array(apply_padding(answers_in_batch,answers2int_dict))\n",
        "  yield padded_question_in_batch,padded_answer_in_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMmCax_pfSiW"
      },
      "source": [
        "training_validation_split = int(len(sorted_out_question)*0.15)\n",
        "\n",
        "training_questions = sorted_out_question[training_validation_split:]\n",
        "training_answers = sorted_out_answers[training_validation_split:]\n",
        "validation_questions = sorted_out_question[:training_validation_split]\n",
        "validation_answers   = sorted_out_answers[:training_validation_split]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgc9Uv2GfVfl"
      },
      "source": [
        "batch_index_check_trainingLoss=100 \n",
        "batch_index_check_validationLoss=(len(training_questions)//(batch_size//2))-1\n",
        "list_validation_loss_error=[]\n",
        "Total_training_loss_error=0\n",
        "Total_validation_loss_error=0\n",
        "early_stopping_check=0\n",
        "early_stopping_stop=1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwG3Z7B7fb81"
      },
      "source": [
        "checkpoint = \"./checkpoints.ckpt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4v0Q5qZfhQ_"
      },
      "source": [
        "session.run(tf.compat.v1.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx6fDrvSfkio"
      },
      "source": [
        "for epoch in range(1,epochs+1):\n",
        "    for batch_index,(padded_question_in_batch,padded_answer_in_batch) in enumerate(splits_into_batches(questions,answers,batch_size)):\n",
        "        start_time=time.time()\n",
        "        _,batch_training_loss_error=session.run([Optimizer_Clipped_Gradient,loss_error],{inputs:padded_question_in_batch, target:padded_answer_in_batch, LR:learning_rate,sequence_length:padded_answer_in_batch.shape[1], keep_prob:keep_probability})\n",
        "        Total_training_loss_error+= batch_training_loss_error\n",
        "        end_time=time.time()\n",
        "        batch_time=end_time-start_time\n",
        "        if batch_index%batch_index_check_trainingLoss==0:\n",
        "            print(\"epoch:{:>3}/{},batch_size:{:>4}/{},training_loss_error:{:>6.3f},training_time_batches:{:d}seconds\".format(epoch,epochs,batch_index,len(training_questions)//batch_size, Total_training_loss_error/batch_index_check_trainingLoss, int(batch_time*batch_index_check_trainingLoss)))\n",
        "            Total_training_loss_error=0\n",
        "            \n",
        "        if batch_index%batch_index_check_validationLoss==0&batch_index>0:\n",
        "            Total_validation_loss_error=0\n",
        "            start_time=time.time()\n",
        "            for batch_validation,(padded_question_in_batch,padded_answer_in_batch) in enumerate(splits_into_batches(validation_questions,validation_answers,batch_size)):\n",
        "                batch_validation_loss_error=session.run(loss_error,{inputs:padded_question_in_batch, target:padded_answer_in_batch, LR:learning_rate,sequence_length:padded_answer_in_batch.shape[1], keep_prob:1})\n",
        "                \n",
        "                Total_validation_loss_error+= batch_validation_loss_error\n",
        "                end_time=time.time()\n",
        "                batch_time=end_time-start_time\n",
        "                avg_validation_loss_error=Total_validation_loss_error/(len(validation_questions)/batch_size)\n",
        "                \n",
        "                print(\"validation_loss_error:{:>6.3f},validation_time_batches:{:d}seconds\".format(avg_validation_loss_error, int(batch_time)))\n",
        "                learning_rate *= learning_rate_decay\n",
        "                \n",
        "                if learning_rate<min_learning_rate:\n",
        "                    learning_rate=min_learning_rate\n",
        "                    list_validation_loss_error.append(avg_validation_loss_error)\n",
        "                \n",
        "                if avg_validation_loss_error<=min(list_validation_loss_error):\n",
        "                    print(\"i speak better now\")\n",
        "                    early_stopping_check=0\n",
        "                    saver=tf.compat.v1.train.Saver()\n",
        "                    saver.save(session,checkpoint)\n",
        "                else:\n",
        "                    print(\"Sorry, I don't speak better, I need to practice more\")\n",
        "                    early_stopping_check+=1\n",
        "                    if early_stopping_check==early_stopping_stop:\n",
        "                        break\n",
        "    if early_stopping_check==early_stopping_stop:\n",
        "        print(\"My apologies, i can't speak better anymore, this is the best i can do\")\n",
        "        break\n",
        "print(\"conversation over\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}