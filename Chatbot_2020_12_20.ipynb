{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot_2020-12-20.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCcyn67HZcSxDlPqM1xRxU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjapan87/ChatbotImplement_V1/blob/main/Chatbot_2020_12_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aUppWBclw6M"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Dec 15 18:16:29 2020\n",
        "\n",
        "@author: Dell\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re \n",
        "import time\n",
        "\n",
        "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]\n",
        "\n",
        "#in id2line just having line pai\n",
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))\n",
        "\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "for conversation in conversations_ids:\n",
        "  for i in range(len(conversation) - 1):\n",
        "    questions.append(id2line[conversation[i]])\n",
        "    answers.append(id2line[conversation[i+1]])\n",
        "\n",
        "\n",
        "\n",
        "#preprocessing function definition\n",
        "def cleanText(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"we'd\", \"we would\", text)    \n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
        "    return text\n",
        "    \n",
        "#preprocessing the \"questions\"\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(cleanText(question))\n",
        "\n",
        "#preprocessing the \"answers\"\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(cleanText(answer))    \n",
        "\n",
        "\n",
        "#create a dictionary that map each word to its no. of occurences\n",
        "#we can use many different kind of models , Bag of words, TDIDF, Word2Vec, BERT etc.\n",
        "#however these models are on lexicological level, but we have sequential data, \n",
        "#so we created this for loop.\n",
        "word2count = {}\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "          word2count[word] = 1\n",
        "        else:\n",
        "          word2count[word] += 1\n",
        "\n",
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "          word2count[word] = 1\n",
        "        else:\n",
        "          word2count[word] += 1    \n",
        "\n",
        "\n",
        "\n",
        "##filtering (aprox.) 5% of extra-words & sorting the occurences to remove repeated entries\n",
        "threshold_ques = 20\n",
        "word_number = 0\n",
        "\n",
        "question2int_dict = {}\n",
        "for word,count in word2count.items():\n",
        "    if count >= threshold_ques:\n",
        "        question2int_dict[word] = word_number\n",
        "        word_number += 1\n",
        "        \n",
        "threshold_ans = 20\n",
        "answers2int_dict = {}\n",
        "for word,count in word2count.items():\n",
        "    if count >= threshold_ans:\n",
        "        answers2int_dict[word] = word_number\n",
        "        word_number += 1    \n",
        "        \n",
        "\n",
        "\n",
        "# Adding 4 tags <EOS> (end of sentence) & <SOS> (start of sentence), <padding>, <out> \n",
        "#<OUT> token was created to replace less-frequent word occurence.\n",
        "    \n",
        "tokens = [\"<PAD>\",\"<EOS>\",\"<OUT>\",\"<SOS>\"]\n",
        "\n",
        "for token in tokens:\n",
        "    question2int_dict[token] = len(question2int_dict)+1\n",
        "    \n",
        "for token in tokens:\n",
        "    answers2int_dict[token] = len(answers2int_dict)+1\n",
        "\n",
        "# Inversing a dictionary key:values --> values:key\n",
        "answers2int_wordDict = {word_i:word for word,word_i in answers2int_dict.items()}    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Adding string token to end of every Clean_answers\n",
        "# those 4 tags added above was at word-level and now we again add the tags to sentence-level.\n",
        "#because decoder needs to understand the starting of decoding process (as identifiers)\n",
        "\n",
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] += \" <EOS>\"\n",
        "\n",
        "\n",
        "\n",
        "# translating all the questions &answers into integer \n",
        "#using the dictionary that was created and replacing the words to occurence in integers.\n",
        "# & replacing all the words that were filtered out by <OUT> tag.\n",
        "\n",
        "\n",
        "out_questions = []\n",
        "#now the words we removed (5% less frequent word occurrence), how to accomodate these\n",
        "#for these <OUT> words, which were removed, and now again looking \n",
        "#at sentence level, we need the <OUT> tag to replace for those missing part. \n",
        "for question in clean_questions:\n",
        "  integ=[] #using full-sentences & using the created dictionarie\n",
        "  for word in question.split():\n",
        "    if word not in question2int_dict:\n",
        "      integ.append(question2int_dict['<OUT>'])\n",
        "    else:\n",
        "      integ.append(question2int_dict[word])\n",
        "  out_questions.append(integ)\n",
        "\n",
        "out_answers = []\n",
        "\n",
        "for answer in clean_answers:\n",
        "  integ=[] #using full-sentences & using the created dictionaries\n",
        "  for word in answer.split():\n",
        "    if word not in answers2int_dict:\n",
        "      integ.append(answers2int_dict['<OUT>'])\n",
        "    else:\n",
        "      integ.append(answers2int_dict[word])\n",
        "  out_answers.append(integ)\n",
        "\n",
        "\n",
        "\n",
        "#sort all the questions & answers\n",
        "#sort the sentences in question2int which have all the questions as integer\n",
        "#for training puropse, so we sorted the clean out_question as ascending order.\n",
        "# starting from 1 word sentence, to many word sentence.\n",
        "#fetch the \n",
        "\n",
        "sorted_out_question = []\n",
        "sorted_out_answers = []\n",
        "for length in range(1, 10+1):\n",
        "    for i in enumerate(out_questions):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_out_question.append(out_questions[i[0]])\n",
        "            sorted_out_answers.append(out_answers[i[0]])\n",
        "\n",
        "\n",
        "#sequences to sequence model\n",
        "\n",
        "##create placeholder (tensors)\n",
        "##input of 4 raw material for building the archtitecture of the model.\n",
        "##input, target, learningrate,keepProb.\n",
        "##in tensorflow all the variables are passed as placeholder(variables in tensorflow)\n",
        "##placeholders are very advanced array.\n",
        "##advanced data-structure\n",
        "\n",
        "def model_input():\n",
        "    inputs = tf.compat.v1.placeholder(tf.int32,[None,None],name = \"input\")\n",
        "    target = tf.compat.v1.placeholder(tf.int32,[None,None],name = \"output\")\n",
        "    LR = tf.compat.v1.placeholder(tf.float32,name = \"learning_rate\")\n",
        "    keep_prob = tf.compat.v1.placeholder(tf.float32,name = \"dropout\")\n",
        "    return inputs,target,learning_rate,keep_prob    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Preprocessing for Decoder as it will predict the answers.\n",
        "##decoder requirement : we require <SOS> in the begining of the sorted_answers\n",
        "##decoder requirement : we require all the sorted_answers in batches\n",
        "\n",
        "def preprocess (target,word2int,batch_size):\n",
        "    left_side = tf.fill([batch_size,1], word2int[\"<SOS>\"], name=None)\n",
        "    ##tf.fill is function is filing an empty array and filling with scalar values\n",
        "    ##tf.fill (left side) will add <SOS>\n",
        "    ##tf.fill (right side) will have all the answers except the last entry (which is <EOS>)\n",
        "    ##Get 2 seperate columns, in the end we join both of these Left & right side together.\n",
        "    ## WORD2INT is a new variable, which indicates each word has a integer value.\n",
        "    \n",
        "    right_side = tf.strided_slice(input = target, \n",
        "                                  begin = [0,0], \n",
        "                                  end = [batch_size,-1], \n",
        "                                  strides=[1,1], \n",
        "                                  begin_mask=0, \n",
        "                                  end_mask=0, \n",
        "                                  ellipsis_mask=0,\n",
        "                                  new_axis_mask=0, \n",
        "                                  shrink_axis_mask=0, \n",
        "                                  var=None, name=None)\n",
        "    pre_process_targets =tf.concat(values = [left_side,right_side], axis = 1)\n",
        "    return pre_process_targets\n",
        "   \n",
        "\n",
        "    \n",
        "#ENCODER\n",
        "##Encoder won't have training or testing data difference.\n",
        "##will work in a similar way for both training/testing.\n",
        "##batches of questions are going to be fed in the encoder\n",
        "##Encoder\"s basic function is to understand the fed text.\n",
        "\n",
        "def encoder (rnn_input,\n",
        "             rnn_size,\n",
        "             num_layers,\n",
        "             keep_prob,\n",
        "             sequence_length):\n",
        "    create_LSTM = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "    LSTM_dropout = tf.contrib.rnn.DropoutWrapper(create_LSTM,input_keep_prob = keep_prob)\n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([LSTM_dropout]*num_layers)\n",
        "    _,encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_forward = encoder_cell, \n",
        "                                                      cell_backwards = encoder_cell,\n",
        "                                                      sequence_length = sequence_length,\n",
        "                                                      inputs = rnn_input,\n",
        "                                                      dtype = tf.float32)\n",
        "    return encoder_state \n",
        "    \n",
        "    #only encoder state will go to the decoder\n",
        "    ##input_keep_prob is a variable to keep track of keep_prob\n",
        "    ##ecoder is going to be stacked layers\n",
        "    ##bidirectional RNN returns 2 parameters, 1st is encoder_states, 2nd not used\n",
        "    ##\n",
        "    \n",
        "    \n",
        "    #tf.contrib is used to create LSTM cells\n",
        "##rnn_input is the input to the RNN.\n",
        "##rnn_size =  how many number of input tensors of the the encoder.\n",
        "##rnn_layers = no. of layers\n",
        "##keep_prob is to apply dropout regularization, which is used to improve to\n",
        "##control drop-out rate(neurons which we choose to overwrite) so that we can \n",
        "##activate, (usually kept at 20% during training iterations).\n",
        "##sometime dropout rates fluctuates (too high, or too low) \n",
        "##so keep_prob will help to control this fluctuation.\n",
        "##sequence_len is the list of length of each question in a batch.\n",
        "\n",
        "\n",
        "\n",
        "#DECODER FOR TRAINING DATA\n",
        "def decoder_training_data (encoder_state, \n",
        "                           decoder_cell, \n",
        "                           decoder_embedding_input, \n",
        "                           sequence_length, \n",
        "                           decoding_scope, \n",
        "                           output_function, \n",
        "                           keep_prob , \n",
        "                           batch_size):\n",
        "    \n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size], \n",
        "                                dtype=tf.dtypes.float32, \n",
        "                                name=None) #initiallization of weights of attention state\n",
        "    \n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function \\\n",
        "        = tf.contrib.seq2seq.prepare_attention(attention_states,\n",
        "                                               attention_option = \"Bahdanau\",\n",
        "                                               num_units = decoder_cell.output_size,\n",
        "                                               reuse=False) \n",
        "    \n",
        "    \n",
        "    #Adding attention features & prepare attention module\n",
        "    ##attention_keys = the keys to be compared to te target state.\n",
        "    ##attention_values = is the value to construct the context vectors. \n",
        "    ##Context is returned by encoder & used by the deceoder.\n",
        "    ##attention_score = is used to compute the similarity between key & target states\n",
        "    ##attention_construct = is used to build the attention states.\n",
        "    ##attention_column is the column of the dataset.\n",
        "    ##decoding_scope will wrap all the variables(contextual_vector, with attention mechanism)\n",
        "    ##Attention are weights, so (not integers but matrix)\n",
        "    ## because there are multiple hidden states, \n",
        "    ##and each hidden layer has a weight attached, so they need to be stored in the form of matrix)\n",
        "    ##attention state, has 3 dimensions, \n",
        "    ##row = batch size, column= 1, elements = output of decoder cell of hidden layer\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ###continued from defining the decoder_training_data\n",
        "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0], \n",
        "                                                                        attention_keys, \n",
        "                                                                        attention_values, \n",
        "                                                                        attention_score_function, \n",
        "                                                                        attention_construct_function, \n",
        "                                                                        name=\"AttentionDecoderTraining\") \n",
        "    #pass all features of attention declared in the previous line of code.\n",
        "    \n",
        "    decoder_output,_,_ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell, \n",
        "                                                                training_decoder_function, \n",
        "                                                                decoder_embedding_input, \n",
        "                                                                sequence_length, \n",
        "                                                                scope = decoding_scope, \n",
        "                                                                name=\"DecoderOutputforTraining\")\n",
        "    decoder_dropout = tf.nn.dropout(decoder_output,\n",
        "                                    keep_prob,\n",
        "                                    name=\"DecoderDropoutController\")\n",
        "    \n",
        "    return output_function(decoder_dropout)\n",
        "#helping to updates the weights as it will retrain with feedback mechanism.\n",
        "\n",
        "\n",
        "#DECODER FOR TESTING DATA\n",
        "\n",
        "##Decoder function of the testing is going to predict the output of the test set.\n",
        "##We will have a Validation set, which will also be given to Training data.\n",
        "##But also Cross-Validation will be done, which is a technique where we keep small part of ou data \n",
        "##to test the predictive power of of model's observation. It is useful to remove overfitting, & \n",
        "##also improves the accuracy on the new observations of the new data.\n",
        "def decoder_testing_data (encoder_state, \n",
        "                          decoder_cell, \n",
        "                          decoder_embedding_matrix, \n",
        "                          sos_id, \n",
        "                          eos_id, \n",
        "                          maximum_length, \n",
        "                          num_words, \n",
        "                          decoding_scope, \n",
        "                          output_function, \n",
        "                          keep_prob , \n",
        "                          batch_size):\n",
        "    \n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size], \n",
        "                                dtype=tf.dtypes.float32, \n",
        "                                name=None) #initiallization of weights of attention state\n",
        "    \n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function \\\n",
        "        = tf.contrib.seq2seq.prepare_attention(attention_states, \n",
        "                                               attention_option = \"bahdanau\", \n",
        "                                               num_units = decoder_cell.output_size, \n",
        "                                               reuse=False) \n",
        "        #Adding attention features & prepare attention module\n",
        "    ## in testing we are making the \"inference\" & this is the main difference. \n",
        "    ##inference is using the brain & reacting to user input.\n",
        "    testing_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function, \n",
        "                                                                           encoder_state[0], \n",
        "                                                                           attention_keys, \n",
        "                                                                           attention_values, \n",
        "                                                                           attention_score_function, \n",
        "                                                                           attention_construct_function, \n",
        "                                                                           decoder_embedding_matrix, \n",
        "                                                                           sos_id, \n",
        "                                                                           eos_id, \n",
        "                                                                           maximum_length, \n",
        "                                                                           num_words, \n",
        "                                                                           dtype=tf.int32, \n",
        "                                                                           name=None) \n",
        "    #maximum_length is the length of longest answer it can find in the batch.\n",
        "    #num_decoder_symbol is the total numbers of words in the answers, we need to the answers2int_wordDict.\n",
        "    ##once the chatbot is trained, a logic inside its brain exist, & therefore, able to deduce logically\n",
        "    ##the answers that is being asked. So  it makes its own logic during the training phases\n",
        "    ##and during the inference phase, it will use this logic and use it during testing\n",
        "    \n",
        "    test_predictions,_,_ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell, \n",
        "                                                                  testing_decoder_function, \n",
        "                                                                  scope = decoding_scope, \n",
        "                                                                  name=\"DecoderOutputforTesting\")\n",
        "    return test_predictions\n",
        "\n",
        "\n",
        "def decoder (decoder_embedding_input,\n",
        "             decoder_embedding_matrix, \n",
        "             encoder_state, \n",
        "             num_words, \n",
        "             rnn_size, \n",
        "             num_layers, \n",
        "             word2int, \n",
        "             keep_prob, \n",
        "             sequence_length, \n",
        "             batch_size):\n",
        "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
        "        create_LSTM = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "        \n",
        "        LSTM_dropout = tf.contrib.rnn.DropoutWrapper(create_LSTM,input_keep_prob = keep_prob)\n",
        "        \n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell([LSTM_dropout]*num_layers)\n",
        "        ##we need to initialize some weights, \n",
        "        ##that will be associated with the neurons of fully-connected layer(LAST LAYER of RNN)\n",
        "        \n",
        "        weights = tf.truncated_normal_initializer(mean=0.0,\n",
        "                                                  stddev=1.0,\n",
        "                                                  seed=None,\n",
        "                                                  dtype=tf.dtypes.float32)\n",
        "        \n",
        "        bias = tf.zeros_initializer()\n",
        "        \n",
        "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
        "                                                                      num_words,\n",
        "                                                                      activation_fn=tf.nn.relu,\n",
        "                                                                      normalizer_fn=None,\n",
        "                                                                      normalizer_params=None,\n",
        "                                                                      weights_initializer= weights,\n",
        "                                                                                      weights_regularizer=None,\n",
        "                                                                                      biases_initializer=bias,\n",
        "                                                                                      biases_regularizer=None,\n",
        "                                                                                      reuse=None,\n",
        "                                                                                      variables_collections=None,\n",
        "                                                                                      outputs_collections=None,\n",
        "                                                                                      trainable=True,\n",
        "                                                                                      scope=decoding_scope)\n",
        "        ##lambda x was used because x can be user defined (text entry in chatbot)\n",
        "        \n",
        "        training_predictions = decoder_training_data (encoder_state, \n",
        "                                                      decoder_cell, \n",
        "                                                      decoder_embedding_input, \n",
        "                                                      sequence_length, \n",
        "                                                      decoding_scope, \n",
        "                                                      output_function,\n",
        "                                                      keep_prob , \n",
        "                                                      batch_size)\n",
        "        decoding_scope.reuse_variables()\n",
        "        \n",
        "        testing_predictions = decoder_testing_data (encoder_state, \n",
        "                                                    decoder_cell, \n",
        "                                                    decoder_embedding_matrix, \n",
        "                                                    word2int[\"<SOS>\"], \n",
        "                                                    word2int[\"<EOS>\"],   \n",
        "                                                    num_words, \n",
        "                                                    sequence_length-1, \n",
        "                                                    decoding_scope, \n",
        "                                                    output_function, \n",
        "                                                    keep_prob , \n",
        "                                                    batch_size)\n",
        "        \n",
        "        return training_predictions,testing_predictions\n",
        "\n",
        "\n",
        "\n",
        "##Combining the Encoder & Decoder (Sequence to Sequence Model)\n",
        "def seq2seq (inputs,\n",
        "             target,\n",
        "             keep_prob,\n",
        "             sequence_length, \n",
        "             batch_size, \n",
        "             questions_number_words,\n",
        "             answers_number_words,\n",
        "             encoder_embedding_size,\n",
        "             decoder_embedding_size,\n",
        "             rnn_size,\n",
        "             num_layers,\n",
        "             question2int_dict):\n",
        "    ## ASSEMBLAGE ## \n",
        "    encoder_embedded_input \\\n",
        "        =tf.keras.layers.Embedding(input_dim = inputs,\n",
        "                                   vocab_size = answers_number_words+1,\n",
        "                                   output_dim = encoder_embedding_size,\n",
        "                                   embeddings_initializer=tf.random_uniform_initializer(minval=0, maxval=1,seed=None)\n",
        "                                   )\n",
        "        \n",
        "\n",
        "    #tf.keras.layers.Embedding(\n",
        "    #input_dim, output_dim, embeddings_initializer='uniform',\n",
        "    #embeddings_regularizer=None, activity_regularizer=None,\n",
        "    #embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs)\n",
        "    \n",
        "    #tensorflow will automatically build\n",
        "    encoder_state = encoder(encoder_embedded_input, \n",
        "                            rnn_size,\n",
        "                            num_layers,\n",
        "                            keep_prob,\n",
        "                            sequence_length)\n",
        "    \n",
        "    pre_process_targets = preprocess(target, question2int_dict, batch_size)\n",
        "    \n",
        "    ##preprocess (target,word2int,batch_size)\n",
        "    decoder_embedding_matrix \\\n",
        "        = tf.Variable(tf.random.uniform([questions_number_words+1,decoder_embedding_size], \n",
        "                                        minval=0, \n",
        "                                        maxval=1, \n",
        "                                        dtype=tf.dtypes.float32, \n",
        "                                        seed=None, \n",
        "                                        name=None))\n",
        "    \n",
        "    decoder_embedding_input = tf.nn.embedding_lookup(decoder_embedding_matrix, \n",
        "                                                     pre_process_targets, \n",
        "                                                     max_norm=None,\n",
        "                                                     name=None)\n",
        "    \n",
        "    training_predictions,testing_predictions \\\n",
        "        = decoder (decoder_embedding_input,\n",
        "                   decoder_embedding_matrix, \n",
        "                   encoder_state, \n",
        "                   questions_number_words, \n",
        "                   rnn_size, \n",
        "                   num_layers, \n",
        "                   question2int_dict, \n",
        "                   keep_prob, \n",
        "                   sequence_length, \n",
        "                   batch_size)\n",
        "    \n",
        "    return training_predictions,testing_predictions\n",
        "\n",
        "\n",
        "#TRAINING THE SEQ2SEQ MODEL\n",
        "\n",
        "##1) Setting the hyperparameter\n",
        "\n",
        "epoch = 10\n",
        "batch_size = 64\n",
        "rnn_size = 512 ##no. of neurons in 1 RNN cell\n",
        "num_layers = 3\n",
        "encoder_embedding_size = 512\n",
        "decoder_embedding_size = 512\n",
        "\n",
        "learning_rate = 0.01 \n",
        "##if this learning is to be too high then model is going to learn to fast, \n",
        "##which isn't optimum (not too high or too low), \n",
        "##the amount by which the weights will be updated aka step-size.(usually 0.0 to 1.0 ) \n",
        "\n",
        "learning_rate_decay = 0.90 \n",
        "## learning_rate is going to be reduced after iteration of the training, \n",
        "##so that the model can be learn deeply the logic.\n",
        "\n",
        "min_learning_rate = 0.0001\n",
        "keep_probability = 0.50 #recommended by geoffery hinton & defined in Tensorflow by same name\n",
        "\n",
        "\n",
        "##2) defining a session, create an object for an interactive session class\n",
        "    ##which is going to open interactive session for interactive flow\n",
        "    ##we also need to reset the tensorflow graph to ensure, that the graph is ready for training \n",
        "    ##graph = in tensorflow a graph is made for all the calling function, \n",
        "    ##and the flow of the calling (return values from 1 block to another)\n",
        "    \n",
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "session = tf.compat.v1.InteractiveSession()\n",
        "\n",
        "##3) taking model input\n",
        "    \n",
        "inputs,target,LR,keep_prob = model_input()\n",
        "\n",
        "##4) setting the sequence length\n",
        "\n",
        "sequence_length = tf.compat.v1.placeholder_with_default(input = 25 , shape = None, name='sequencelength')\n",
        "\n",
        "##5) Defining the Input Tensors\n",
        "\n",
        "input_shape = tf.shape(inputs, out_type=tf.dtypes.int32, name=None)\n",
        "\n",
        "len_question = len(question2int_dict)\n",
        "len_answer = len(answers2int_dict)\n",
        "\n",
        "##6) Getting the training & test predictions\n",
        "training_predictions,testing_predictions\\\n",
        "    =seq2seq (tf.reverse(inputs,[-1]),\n",
        "              target,\n",
        "              keep_prob,\n",
        "              batch_size, \n",
        "              sequence_length,\n",
        "              len_question,\n",
        "              len_answer,\n",
        "              encoder_embedding_size,\n",
        "              decoder_embedding_size,\n",
        "              rnn_size,\n",
        "              num_layers,\n",
        "              question2int_dict)\n",
        " \n",
        "\n",
        "##7) Optimizations; Setting up the Loss, Error, Optimizer & Gradient Clipping \n",
        "##Gradient Clipping (clip between Max & Min  value to avoid some Exploding & Vanishing Gradients).\n",
        "##Optimizer = Adam Optimizer best for Stochastic Gradient & then apply the Gradient Clipping on it.\n",
        "## Loss = Difference between Prediction (Given by Model) & Target (Given by Data-Set)\n",
        "## error & Loss are synonyms (almost same definition)\n",
        "\n",
        "with tf.name_scope(\"Optimizations\"):\n",
        "  loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
        "                                                target,\n",
        "                                                tf.ones[(input_shape[0],sequence_length)],\n",
        "                                                )\n",
        "  \n",
        "  Optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "  Gradient = Optimizer.compute_gradients(loss_error)\n",
        "\n",
        "  Clipped_Gradient = [(tf.clip_by_value(grad_tensor,-5.0,5.0),grad_variable) for grad_tensor,grad_variable in Gradient if grad_tensor is not None]\n",
        "  #grad_tensor & grad_variable are 2 different things. \n",
        "\n",
        "  Optimizer_Clipped_Gradient = Optimizer.apply_gradients(Clipped_Gradient)\n",
        "\n",
        "\n",
        "##8) Pad the sequences with <PAD> token, for equalizing all the sentences of same length.\n",
        "##for each senence of batch, should be of same length.\n",
        "\n",
        "def apply_padding(batch_of_sequences, word2int):\n",
        "  maximum_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
        "  return [sequence + [word2int[\"<PAD>\"]]*(maximum_sequence_length-len(sequence)) for sequence in batch_of_sequences]\n",
        "\n",
        "##we want to see the maximum sequence length, \n",
        "##because we want to apply the padding to the sequences, \n",
        "##because it depends on the maximum length of longest sentence in the batch.\n",
        "##padding also reduces the training time.\n",
        "\n",
        "##9) Splitting the data into batches of batches of questions and answers\n",
        "##Batches are padded sequences of question & answers,\n",
        "## here we are going  to create a new function, and during training apply this function to create the batches\n",
        "## & feed the neural networks with these inputs i.e. questsion & (targets)answers.\n",
        "\n",
        "def splits_into_batches(questions,answers,batch_size): ####CHECK INDENTATION\n",
        "  for batch_index in range(0,len(questions)//batch_size):\n",
        "      #a lot of batches, depending on the total no. of questions, we are going to calculate the total number of batches.\n",
        "      start_index = batch_index*batch_size\n",
        "      #inside the batch, the index of the first questions in the batch.\n",
        "      question_in_batch = questions[start_index:start_index+batch_size]\n",
        "      answers_in_batch  = answers[start_index:start_index+batch_size]\n",
        "      padded_question_in_batch = np.array(apply_padding(question_in_batch, question2int_dict))\n",
        "      padded_answer_in_batch = np.array(apply_padding(answers_in_batch,answers2int_dict))\n",
        "  yield padded_question_in_batch,padded_answer_in_batch\n",
        "\n",
        "##10) Splitting the question & answer in training & validation set.\n",
        "\n",
        "##Validation Split\n",
        "##cross-validation is a technique of ML which is keep aside 10% data from training data-set, but don't feed it during traing,\n",
        "##then use that 10% of kept data to check the predictive power of the ML model.\n",
        "\n",
        "training_validation_split = int(len(sorted_out_question)*0.15)\n",
        "\n",
        "training_questions = sorted_out_question[training_validation_split:]\n",
        "training_answers = sorted_out_answers[training_validation_split:]\n",
        "validation_questions = sorted_out_question[:training_validation_split]\n",
        "validation_answers   = sorted_out_answers[:training_validation_split]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6EogUrN9khs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}